{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a537ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == \"notebooks\":\n",
    "    os.chdir(cwd.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886cccb",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EmoBox.EmoBox import EmoDataset, EmoEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since there is no official valid data, use random split for train valid split, with a ratio of [80, 20]\n",
      "load in 890 samples, only 890 exists in data dir EmoBox/data/\n",
      "load in 291 samples, only 291 exists in data dir EmoBox/data/\n",
      "Num. training samples 890\n",
      "Num. valid samples 0\n",
      "Num. test samples 291\n",
      "Using label_map {'Disgust': 'Disgust', 'Excited': 'Excited', 'Surprise': 'Surprise', 'Sad': 'Sad', 'Sarcastic': 'Sarcastic', 'Happy': 'Happy', 'Neutral': 'Neutral', 'Angry': 'Angry'}\n",
      "since there is no official valid data, use random split for train valid split, with a ratio of [80, 20]\n",
      "load in 890 samples, only 890 exists in data dir EmoBox/data/\n",
      "load in 291 samples, only 291 exists in data dir EmoBox/data/\n",
      "Num. training samples 890\n",
      "Num. valid samples 0\n",
      "Num. test samples 291\n",
      "Using label_map {'Disgust': 'Disgust', 'Excited': 'Excited', 'Surprise': 'Surprise', 'Sad': 'Sad', 'Sarcastic': 'Sarcastic', 'Happy': 'Happy', 'Neutral': 'Neutral', 'Angry': 'Angry'}\n"
     ]
    }
   ],
   "source": [
    "dataset = \"ravdess\"\n",
    "fold = 1  # different datasets have different number of folds, which can be find in data/\n",
    "user_data_dir = \"./\" # path to EmoBox - FIXED: Changed from \"Emobox\" to \"EmoBox\"\n",
    "meta_data_dir = \"EmoBox/data/\" # path to data folder - FIXED: Changed from \"Emobox\" to \"EmoBox\"\n",
    "\n",
    "train = EmoDataset(dataset, user_data_dir, meta_data_dir, fold=fold, split=\"train\")\n",
    "test = EmoDataset(dataset, user_data_dir, meta_data_dir, fold=fold, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e3bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/dasaro/research/FairMLLM-Emotion-Recognition/EmoBox/EmoBox/EmoDataset.py:154: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  wav, sr = librosa.load(wav_path, sr=None, mono=mono)\n",
      "/nfs/home/dasaro/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening 'downloads/emns/cleaned_webm/recorded_audio_jBSPPAn.webm': Format not recognised.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m sample\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/EmoBox/EmoBox/EmoDataset.py:190\u001b[0m, in \u001b[0;36mEmoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    188\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_list[idx]\n\u001b[1;32m    189\u001b[0m key \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \n\u001b[0;32m--> 190\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mread_wav\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memo\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    192\u001b[0m sensitive_attr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msensitive_attr\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/EmoBox/EmoBox/EmoDataset.py:154\u001b[0m, in \u001b[0;36mread_wav\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    146\u001b[0m     wav, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    147\u001b[0m         wav_path, \n\u001b[1;32m    148\u001b[0m         sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m         mono\u001b[38;5;241m=\u001b[39mmono\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     wav, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmono\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wav\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    157\u001b[0m     wav \u001b[38;5;241m=\u001b[39m wav\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/decorator.py:235\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    234\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/librosa/util/decorators.py:63\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/librosa/core/audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/audioread/__init__.py:131\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
      "\u001b[0;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample = test[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2c345f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Angry', 'Surprise', 'Sad', 'Disgust', 'Happy', 'Calm', 'Neutral', 'Fear'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label_map.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95bb1623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Disgust': 32,\n",
       "         'Fear': 32,\n",
       "         'Surprise': 32,\n",
       "         'Calm': 32,\n",
       "         'Happy': 32,\n",
       "         'Angry': 32,\n",
       "         'Sad': 32,\n",
       "         'Neutral': 16})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels =  [data['label'] for data in test]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ecf386",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03cbe8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/dasaro/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Fetching 5 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 69672.82it/s]\n",
      "Loading weights: 100%|██████████████████████████████████████████████████████| 876/876 [00:02<00:00, 297.49it/s, Materializing param=multi_modal_projector.linear.weight]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mllm_emotion_classifier.models import ModelFactory\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ModelFactory.create(\n",
    "    name=\"qwen2-audio-instruct\",\n",
    "    class_labels=set(train.label_map.values()),\n",
    "    do_sample=False,\n",
    "    prompt_name=\"user_labels\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b459f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    collate_fn=model.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f599406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# predictions, labels = [], []\n",
    "# i = 0\n",
    "# for inputs, lbl in tqdm(data_loader, total=len(data_loader)):\n",
    "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "#     preds = model.predict(inputs)\n",
    "#     predictions.extend(preds)\n",
    "#     labels.extend(lbl)\n",
    "#     i += 1\n",
    "#     if i == 100: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d32b4",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7045a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating qwen2-audio-instruct on ravdess\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|                                                                                                                                 | 0/60 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Inference: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:26<00:00,  2.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2026-01-01 20:08:47',\n",
       " 'dataset': 'ravdess',\n",
       " 'model_name': 'qwen2-audio-instruct',\n",
       " 'fold': None,\n",
       " 'num_samples': 240,\n",
       " 'valid_rate': 1.0,\n",
       " 'class_labels': ['Happy',\n",
       "  'Calm',\n",
       "  'Disgust',\n",
       "  'Fear',\n",
       "  'Angry',\n",
       "  'Surprise',\n",
       "  'Neutral',\n",
       "  'Sad'],\n",
       " 'metrics': {'global': {'f1_macro': 0.5797,\n",
       "   'f1_weighted': 0.5798,\n",
       "   'accuracy_unweighted': 0.6367,\n",
       "   'accuracy_weighted': 0.6333,\n",
       "   'precision_macro': 0.6369,\n",
       "   'precision_weighted': 0.646,\n",
       "   'recall_macro': 0.6367,\n",
       "   'recall_weighted': 0.6333},\n",
       "  'classwise': {'accuracy': {'Angry': 0.975,\n",
       "    'Calm': 0.8667,\n",
       "    'Disgust': 0.8708,\n",
       "    'Fear': 0.9333,\n",
       "    'Happy': 0.9583,\n",
       "    'Neutral': 0.9333,\n",
       "    'Sad': 0.8375,\n",
       "    'Surprise': 0.8917},\n",
       "   'false_positive_rate': {'Angry': 0.0,\n",
       "    'Calm': 0.0,\n",
       "    'Disgust': 0.149,\n",
       "    'Fear': 0.0385,\n",
       "    'Happy': 0.0144,\n",
       "    'Neutral': 0.0491,\n",
       "    'Sad': 0.1683,\n",
       "    'Surprise': 0.0},\n",
       "   'false_negative_rate': {'Angry': 0.1875,\n",
       "    'Calm': 1.0,\n",
       "    'Disgust': 0.0,\n",
       "    'Fear': 0.25,\n",
       "    'Happy': 0.2188,\n",
       "    'Neutral': 0.3125,\n",
       "    'Sad': 0.125,\n",
       "    'Surprise': 0.8125},\n",
       "   'true_positive_rate': {'Angry': 0.8125,\n",
       "    'Calm': 0.0,\n",
       "    'Disgust': 1.0,\n",
       "    'Fear': 0.75,\n",
       "    'Happy': 0.7812,\n",
       "    'Neutral': 0.6875,\n",
       "    'Sad': 0.875,\n",
       "    'Surprise': 0.1875},\n",
       "   'true_negative_rate': {'Angry': 1.0,\n",
       "    'Calm': 1.0,\n",
       "    'Disgust': 0.851,\n",
       "    'Fear': 0.9615,\n",
       "    'Happy': 0.9856,\n",
       "    'Neutral': 0.9509,\n",
       "    'Sad': 0.8317,\n",
       "    'Surprise': 1.0},\n",
       "   'positive_predictive_value': {'Angry': 1.0,\n",
       "    'Calm': 0.0,\n",
       "    'Disgust': 0.5079,\n",
       "    'Fear': 0.75,\n",
       "    'Happy': 0.8929,\n",
       "    'Neutral': 0.5,\n",
       "    'Sad': 0.4444,\n",
       "    'Surprise': 1.0},\n",
       "   'negative_predictive_value': {'Angry': 0.972,\n",
       "    'Calm': 0.8667,\n",
       "    'Disgust': 1.0,\n",
       "    'Fear': 0.9615,\n",
       "    'Happy': 0.967,\n",
       "    'Neutral': 0.9771,\n",
       "    'Sad': 0.9774,\n",
       "    'Surprise': 0.8889},\n",
       "   'f1_score': {'Angry': 0.8966,\n",
       "    'Calm': 0.0,\n",
       "    'Disgust': 0.6737,\n",
       "    'Fear': 0.75,\n",
       "    'Happy': 0.8333,\n",
       "    'Neutral': 0.5789,\n",
       "    'Sad': 0.5895,\n",
       "    'Surprise': 0.3158}},\n",
       "  'gender': {'Female': {'global': {'f1_macro': 0.5203,\n",
       "     'f1_weighted': 0.5106,\n",
       "     'accuracy_unweighted': 0.5938,\n",
       "     'accuracy_weighted': 0.5833,\n",
       "     'precision_macro': 0.5396,\n",
       "     'precision_weighted': 0.5356,\n",
       "     'recall_macro': 0.5938,\n",
       "     'recall_weighted': 0.5833},\n",
       "    'classwise': {'accuracy': {'Angry': 0.9333,\n",
       "      'Calm': 0.8667,\n",
       "      'Disgust': 0.8167,\n",
       "      'Fear': 0.95,\n",
       "      'Happy': 0.9667,\n",
       "      'Neutral': 0.95,\n",
       "      'Sad': 0.8167,\n",
       "      'Surprise': 0.8667},\n",
       "     'false_positive_rate': {'Angry': 0.0,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.2115,\n",
       "      'Fear': 0.0,\n",
       "      'Happy': 0.0192,\n",
       "      'Neutral': 0.0357,\n",
       "      'Sad': 0.2115,\n",
       "      'Surprise': 0.0},\n",
       "     'false_negative_rate': {'Angry': 0.5,\n",
       "      'Calm': 1.0,\n",
       "      'Disgust': 0.0,\n",
       "      'Fear': 0.375,\n",
       "      'Happy': 0.125,\n",
       "      'Neutral': 0.25,\n",
       "      'Sad': 0.0,\n",
       "      'Surprise': 1.0},\n",
       "     'true_positive_rate': {'Angry': 0.5,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 1.0,\n",
       "      'Fear': 0.625,\n",
       "      'Happy': 0.875,\n",
       "      'Neutral': 0.75,\n",
       "      'Sad': 1.0,\n",
       "      'Surprise': 0.0},\n",
       "     'true_negative_rate': {'Angry': 1.0,\n",
       "      'Calm': 1.0,\n",
       "      'Disgust': 0.7885,\n",
       "      'Fear': 1.0,\n",
       "      'Happy': 0.9808,\n",
       "      'Neutral': 0.9643,\n",
       "      'Sad': 0.7885,\n",
       "      'Surprise': 1.0},\n",
       "     'positive_predictive_value': {'Angry': 1.0,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.4211,\n",
       "      'Fear': 1.0,\n",
       "      'Happy': 0.875,\n",
       "      'Neutral': 0.6,\n",
       "      'Sad': 0.4211,\n",
       "      'Surprise': 0.0},\n",
       "     'negative_predictive_value': {'Angry': 0.9286,\n",
       "      'Calm': 0.8667,\n",
       "      'Disgust': 1.0,\n",
       "      'Fear': 0.9455,\n",
       "      'Happy': 0.9808,\n",
       "      'Neutral': 0.9818,\n",
       "      'Sad': 1.0,\n",
       "      'Surprise': 0.8667},\n",
       "     'f1_score': {'Angry': 0.6667,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.5926,\n",
       "      'Fear': 0.7692,\n",
       "      'Happy': 0.875,\n",
       "      'Neutral': 0.6667,\n",
       "      'Sad': 0.5926,\n",
       "      'Surprise': 0.0}}},\n",
       "   'Male': {'global': {'f1_macro': 0.5957,\n",
       "     'f1_weighted': 0.5986,\n",
       "     'accuracy_unweighted': 0.651,\n",
       "     'accuracy_weighted': 0.65,\n",
       "     'precision_macro': 0.6343,\n",
       "     'precision_weighted': 0.6452,\n",
       "     'recall_macro': 0.651,\n",
       "     'recall_weighted': 0.65},\n",
       "    'classwise': {'accuracy': {'Angry': 0.9889,\n",
       "      'Calm': 0.8667,\n",
       "      'Disgust': 0.8889,\n",
       "      'Fear': 0.9278,\n",
       "      'Happy': 0.9556,\n",
       "      'Neutral': 0.9278,\n",
       "      'Sad': 0.8444,\n",
       "      'Surprise': 0.9},\n",
       "     'false_positive_rate': {'Angry': 0.0,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.1282,\n",
       "      'Fear': 0.0513,\n",
       "      'Happy': 0.0128,\n",
       "      'Neutral': 0.0536,\n",
       "      'Sad': 0.1538,\n",
       "      'Surprise': 0.0},\n",
       "     'false_negative_rate': {'Angry': 0.0833,\n",
       "      'Calm': 1.0,\n",
       "      'Disgust': 0.0,\n",
       "      'Fear': 0.2083,\n",
       "      'Happy': 0.25,\n",
       "      'Neutral': 0.3333,\n",
       "      'Sad': 0.1667,\n",
       "      'Surprise': 0.75},\n",
       "     'true_positive_rate': {'Angry': 0.9167,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 1.0,\n",
       "      'Fear': 0.7917,\n",
       "      'Happy': 0.75,\n",
       "      'Neutral': 0.6667,\n",
       "      'Sad': 0.8333,\n",
       "      'Surprise': 0.25},\n",
       "     'true_negative_rate': {'Angry': 1.0,\n",
       "      'Calm': 1.0,\n",
       "      'Disgust': 0.8718,\n",
       "      'Fear': 0.9487,\n",
       "      'Happy': 0.9872,\n",
       "      'Neutral': 0.9464,\n",
       "      'Sad': 0.8462,\n",
       "      'Surprise': 1.0},\n",
       "     'positive_predictive_value': {'Angry': 1.0,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.5455,\n",
       "      'Fear': 0.7037,\n",
       "      'Happy': 0.9,\n",
       "      'Neutral': 0.4706,\n",
       "      'Sad': 0.4545,\n",
       "      'Surprise': 1.0},\n",
       "     'negative_predictive_value': {'Angry': 0.9873,\n",
       "      'Calm': 0.8667,\n",
       "      'Disgust': 1.0,\n",
       "      'Fear': 0.9673,\n",
       "      'Happy': 0.9625,\n",
       "      'Neutral': 0.9755,\n",
       "      'Sad': 0.9706,\n",
       "      'Surprise': 0.8966},\n",
       "     'f1_score': {'Angry': 0.9565,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.7059,\n",
       "      'Fear': 0.7451,\n",
       "      'Happy': 0.8182,\n",
       "      'Neutral': 0.5517,\n",
       "      'Sad': 0.5882,\n",
       "      'Surprise': 0.4}}}},\n",
       "  'language': {'English': {'global': {'f1_macro': 0.5797,\n",
       "     'f1_weighted': 0.5798,\n",
       "     'accuracy_unweighted': 0.6367,\n",
       "     'accuracy_weighted': 0.6333,\n",
       "     'precision_macro': 0.6369,\n",
       "     'precision_weighted': 0.646,\n",
       "     'recall_macro': 0.6367,\n",
       "     'recall_weighted': 0.6333},\n",
       "    'classwise': {'accuracy': {'Angry': 0.975,\n",
       "      'Calm': 0.8667,\n",
       "      'Disgust': 0.8708,\n",
       "      'Fear': 0.9333,\n",
       "      'Happy': 0.9583,\n",
       "      'Neutral': 0.9333,\n",
       "      'Sad': 0.8375,\n",
       "      'Surprise': 0.8917},\n",
       "     'false_positive_rate': {'Angry': 0.0,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.149,\n",
       "      'Fear': 0.0385,\n",
       "      'Happy': 0.0144,\n",
       "      'Neutral': 0.0491,\n",
       "      'Sad': 0.1683,\n",
       "      'Surprise': 0.0},\n",
       "     'false_negative_rate': {'Angry': 0.1875,\n",
       "      'Calm': 1.0,\n",
       "      'Disgust': 0.0,\n",
       "      'Fear': 0.25,\n",
       "      'Happy': 0.2188,\n",
       "      'Neutral': 0.3125,\n",
       "      'Sad': 0.125,\n",
       "      'Surprise': 0.8125},\n",
       "     'true_positive_rate': {'Angry': 0.8125,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 1.0,\n",
       "      'Fear': 0.75,\n",
       "      'Happy': 0.7812,\n",
       "      'Neutral': 0.6875,\n",
       "      'Sad': 0.875,\n",
       "      'Surprise': 0.1875},\n",
       "     'true_negative_rate': {'Angry': 1.0,\n",
       "      'Calm': 1.0,\n",
       "      'Disgust': 0.851,\n",
       "      'Fear': 0.9615,\n",
       "      'Happy': 0.9856,\n",
       "      'Neutral': 0.9509,\n",
       "      'Sad': 0.8317,\n",
       "      'Surprise': 1.0},\n",
       "     'positive_predictive_value': {'Angry': 1.0,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.5079,\n",
       "      'Fear': 0.75,\n",
       "      'Happy': 0.8929,\n",
       "      'Neutral': 0.5,\n",
       "      'Sad': 0.4444,\n",
       "      'Surprise': 1.0},\n",
       "     'negative_predictive_value': {'Angry': 0.972,\n",
       "      'Calm': 0.8667,\n",
       "      'Disgust': 1.0,\n",
       "      'Fear': 0.9615,\n",
       "      'Happy': 0.967,\n",
       "      'Neutral': 0.9771,\n",
       "      'Sad': 0.9774,\n",
       "      'Surprise': 0.8889},\n",
       "     'f1_score': {'Angry': 0.8966,\n",
       "      'Calm': 0.0,\n",
       "      'Disgust': 0.6737,\n",
       "      'Fear': 0.75,\n",
       "      'Happy': 0.8333,\n",
       "      'Neutral': 0.5789,\n",
       "      'Sad': 0.5895,\n",
       "      'Surprise': 0.3158}}}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mllm_emotion_classifier.evaluate import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840f8093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_macro': 0.5797,\n",
       " 'f1_weighted': 0.5798,\n",
       " 'accuracy_unweighted': 0.6367,\n",
       " 'accuracy_weighted': 0.6333,\n",
       " 'precision_macro': 0.6369,\n",
       " 'precision_weighted': 0.646,\n",
       " 'recall_macro': 0.6367,\n",
       " 'recall_weighted': 0.6333}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.results['metrics']['global']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
