{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a537ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == \"notebooks\":\n",
    "    os.chdir(cwd.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886cccb",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EmoBox.EmoBox import EmoDataset, EmoEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9117126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since there is no official valid data, use random split for train valid split, with a ratio of [80, 20]\n",
      "load in 4446 samples, only 4446 exists in data dir EmoBox/data/\n",
      "load in 1085 samples, only 1085 exists in data dir EmoBox/data/\n",
      "Num. training samples 4446\n",
      "Num. valid samples 0\n",
      "Num. test samples 1085\n",
      "Using label_map {'neu': 'Neutral', 'hap': 'Happy', 'ang': 'Angry', 'sad': 'Sad', 'exc': 'Happy'}\n",
      "since there is no official valid data, use random split for train valid split, with a ratio of [80, 20]\n",
      "load in 4446 samples, only 4446 exists in data dir EmoBox/data/\n",
      "load in 1085 samples, only 1085 exists in data dir EmoBox/data/\n",
      "Num. training samples 4446\n",
      "Num. valid samples 0\n",
      "Num. test samples 1085\n",
      "Using label_map {'neu': 'Neutral', 'hap': 'Happy', 'ang': 'Angry', 'sad': 'Sad', 'exc': 'Happy'}\n"
     ]
    }
   ],
   "source": [
    "dataset = \"iemocap\"\n",
    "fold = 1  # different datasets have different number of folds, which can be find in data/\n",
    "user_data_dir = \"./\" # path to EmoBox - FIXED: Changed from \"Emobox\" to \"EmoBox\"\n",
    "meta_data_dir = \"EmoBox/data/\" # path to data folder - FIXED: Changed from \"Emobox\" to \"EmoBox\"\n",
    "label2idx = {'hap':0, 'sad':1, 'ang':2, 'neu':3} # you may need to define a label to index mapping for your own training, see `data/iemocap/label_map.json`\n",
    "\n",
    "train = EmoDataset(dataset, user_data_dir, meta_data_dir, fold=fold, split=\"train\")\n",
    "test = EmoDataset(dataset, user_data_dir, meta_data_dir, fold=fold, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e3bfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "id = 102\n",
    "audio_1 = test[id]['audio']\n",
    "audio_2 = test[id]['audio']\n",
    "np.array_equal(audio_1, audio_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2c345f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Neutral', 'Happy', 'Angry', 'Sad', 'Happy'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label_map.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95bb1623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Neutral': 384, 'Happy': 278, 'Angry': 229, 'Sad': 194})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels =  [data['label'] for data in test]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1a863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ecf386",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9589555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|audio_bos|><|AUDIO|><|audio_eos|>Classify the speaker’s tone in the audio. Select one of: {'N': 'Neutral', 'H': 'Happy', 'A': 'Angry', 'S': 'Sad'}. Answer:\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUDIO_PROMPT_TEMPLATE = (\n",
    "    \"<|audio_bos|><|AUDIO|><|audio_eos|>\"\n",
    "    \"Classify the speaker’s tone in the audio. \"\n",
    "    \"Select one of: {labels}. \"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "class_labels = test.label_map.values()\n",
    "# letter_to_label = {label[0].upper(): label for label in class_labels}\n",
    "# label_to_letter = {label: label[0].upper() for label in class_labels}\n",
    "# label_options = \", \".join([f\"{label_to_letter[label]}: {label}\" for label in class_labels])\n",
    "label_dict = {label[0]: label for label in class_labels}\n",
    "AUDIO_PROMPT_TEMPLATE = AUDIO_PROMPT_TEMPLATE.format(labels=label_dict)\n",
    "AUDIO_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cbe8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/dasaro/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mllm_emotion_classifier.models import ModelFactory\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ModelFactory.create(\n",
    "    name=\"qwen2-audio-instruct\",\n",
    "    # checkpoint=\"Qwen/Qwen2-Audio-7B\",\n",
    "    # checkpoint=\"Qwen/Qwen2-Audio-7B-Instruct\",\n",
    "    class_labels=set(train.label_map.values()),\n",
    "    do_sample=False,\n",
    "    device=device,\n",
    "    prompt_name=\"user_labels\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b459f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    collate_fn=model.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f599406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                         | 0/272 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword argument `audios` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `audios` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `audios` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `audios` is not a valid argument for this processor and will be ignored.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  1%|██▏                                                                                                                                              | 4/272 [00:01<01:20,  3.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "predictions, labels = [], []\n",
    "i = 0\n",
    "for inputs, lbl in tqdm(data_loader, total=len(data_loader)):\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    preds = model.predict(inputs)\n",
    "    predictions.extend(preds)\n",
    "    labels.extend(lbl)\n",
    "    i += 1\n",
    "    if len(predictions) >= 20:\n",
    "        break\n",
    "\n",
    "# new_data = pd.DataFrame({\n",
    "#     \"label\": labels,\n",
    "#     \"prediction\": predictions,\n",
    "# })\n",
    "# csv_path = \"notebooks/qwen2-audio-iemocap-fold1-predictions-json.csv\"\n",
    "\n",
    "# if os.path.exists(csv_path):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     last_digit = df.columns[-1][0]\n",
    "#     next_id = int(last_digit) + 1\n",
    "#     df[f\"{next_id}_prediction\"] = new_data[\"prediction\"]\n",
    "#     print(f\"Added columns with ID {next_id} to existing CSV\")\n",
    "# else:\n",
    "#     df = pd.DataFrame({\n",
    "#         \"label\": new_data[\"label\"],\n",
    "#         \"0_prediction\": new_data[\"prediction\"],\n",
    "#     })\n",
    "#     print(f\"Created new CSV with ID 0\")\n",
    "\n",
    "# df.to_csv(csv_path, index=False)\n",
    "# print(f\"Saved to {csv_path}\")\n",
    "# print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6667f7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral',\n",
       " 'Neutral']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121f4692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '0', 'F1 Macro': 0.7043, 'F1 Weighted': 0.7183},\n",
       " {'ID': '1', 'F1 Macro': 0.6647, 'F1 Weighted': 0.717},\n",
       " {'ID': '2', 'F1 Macro': 0.766, 'F1 Weighted': 0.7892}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "csv_path = \"notebooks/qwen2-audio-iemocap-fold1-predictions-subset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "prediction_cols = [col for col in df.columns if col.endswith(\"_prediction\")]\n",
    "y_true = df[\"label\"]\n",
    "\n",
    "results = []\n",
    "for pred_col in prediction_cols:\n",
    "    pred_id = pred_col.split(\"_\")[0]\n",
    "    y_pred = df[pred_col].dropna()\n",
    "    y_true_valid = y_true[df[pred_col].notna()]\n",
    "    \n",
    "    f1_macro = f1_score(y_true_valid, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true_valid, y_pred, average='weighted')\n",
    "    \n",
    "    results.append({\n",
    "        'ID': pred_id,\n",
    "        'F1 Macro': round(f1_macro, 4),\n",
    "        'F1 Weighted': round(f1_weighted, 4)\n",
    "    })\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d32b4",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7045a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating Qwen2-Audio-7B-Instruct on iemocap\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|                                                                                                         | 0/272 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  45%|██████████████████████████████████████████▉                                                    | 123/272 [00:53<01:11,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: could not parse response \"Wakeful\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 272/272 [01:55<00:00,  2.36it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmllm_emotion_classifier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Evaluator\n\u001b[1;32m      3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/src/mllm_emotion_classifier/evaluate/evaluate.py:77\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, model, dataloader, n_samples, fold)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     76\u001b[0m y_true, y_pred, sens_attr_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_predictions(model, dataloader, n_samples\u001b[38;5;241m=\u001b[39mn_samples)\n\u001b[0;32m---> 77\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msens_attr_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_true \u001b[38;5;241m=\u001b[39m y_true\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_pred \u001b[38;5;241m=\u001b[39m y_pred\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/src/mllm_emotion_classifier/evaluate/evaluate.py:53\u001b[0m, in \u001b[0;36mEvaluator._compute_metrics\u001b[0;34m(self, y_true, y_pred, sens_attr_dict)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compute_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred, sens_attr_dict):\n\u001b[1;32m     51\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_obj \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationMetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_obj\u001b[38;5;241m.\u001b[39mcompute(sens_attr_dict\u001b[38;5;241m=\u001b[39msens_attr_dict)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sens_attr_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/src/mllm_emotion_classifier/evaluate/metrics.py:18\u001b[0m, in \u001b[0;36mClassificationMetrics.__init__\u001b[0;34m(self, y_true, y_pred, class_labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_labels \u001b[38;5;241m=\u001b[39m class_labels \u001b[38;5;28;01mif\u001b[39;00m class_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfusion_matrices \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_confusion_matrix(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses}\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_confusion_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:466\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    372\u001b[0m     {\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m ):\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mattach_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/sklearn/utils/_unique.py:56\u001b[0m, in \u001b[0;36mattach_unique\u001b[0;34m(return_tuple, *ys)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattach_unique\u001b[39m(\u001b[38;5;241m*\u001b[39mys, return_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attach unique values of ys to ys and return the results.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    The result is a view of y, and the metadata (unique) is not attached to y.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        Input data with unique values attached.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_attach_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_tuple:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/sklearn/utils/_unique.py:56\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattach_unique\u001b[39m(\u001b[38;5;241m*\u001b[39mys, return_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attach unique values of ys to ys and return the results.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    The result is a view of y, and the metadata (unique) is not attached to y.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        Input data with unique values attached.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43m_attach_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_tuple:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/sklearn/utils/_unique.py:23\u001b[0m, in \u001b[0;36m_attach_unique\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m unique \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m unique_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(y\u001b[38;5;241m.\u001b[39mdtype, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique\u001b[39m\u001b[38;5;124m\"\u001b[39m: unique})\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mview(dtype\u001b[38;5;241m=\u001b[39munique_dtype)\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/numpy/lib/_arraysetops_impl.py:286\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    284\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/research/FairMLLM-Emotion-Recognition/.venv/lib/python3.10/site-packages/numpy/lib/_arraysetops_impl.py:353\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis)\u001b[0m\n\u001b[1;32m    351\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    355\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from mllm_emotion_classifier.evaluate import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "840f8093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7926,\n",
       " 'false_positive_rate': 0.1296,\n",
       " 'false_negative_rate': 0.3504,\n",
       " 'true_positive_rate': 0.6496,\n",
       " 'true_negative_rate': 0.8704,\n",
       " 'positive_predictive_value': 0.7376,\n",
       " 'negative_predictive_value': 0.8779,\n",
       " 'f1_score': 0.6141}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.results['metrics']['global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e4f66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': {'Angry': 0.9364,\n",
       "  'Happy': 0.8756,\n",
       "  'Neutral': 0.718,\n",
       "  'Sad': 0.6406},\n",
       " 'false_positive_rate': {'Angry': 0.0035,\n",
       "  'Happy': 0.0322,\n",
       "  'Neutral': 0.0471,\n",
       "  'Sad': 0.4355},\n",
       " 'false_negative_rate': {'Angry': 0.2882,\n",
       "  'Happy': 0.3921,\n",
       "  'Neutral': 0.7109,\n",
       "  'Sad': 0.0103},\n",
       " 'true_positive_rate': {'Angry': 0.7118,\n",
       "  'Happy': 0.6079,\n",
       "  'Neutral': 0.2891,\n",
       "  'Sad': 0.9897},\n",
       " 'true_negative_rate': {'Angry': 0.9965,\n",
       "  'Happy': 0.9678,\n",
       "  'Neutral': 0.9529,\n",
       "  'Sad': 0.5645},\n",
       " 'positive_predictive_value': {'Angry': 0.9819,\n",
       "  'Happy': 0.8667,\n",
       "  'Neutral': 0.7708,\n",
       "  'Sad': 0.331},\n",
       " 'negative_predictive_value': {'Angry': 0.9282,\n",
       "  'Happy': 0.8775,\n",
       "  'Neutral': 0.7099,\n",
       "  'Sad': 0.996},\n",
       " 'f1_score': {'Angry': 0.8253,\n",
       "  'Happy': 0.7146,\n",
       "  'Neutral': 0.4205,\n",
       "  'Sad': 0.4961}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.results['metrics']['classwise']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
